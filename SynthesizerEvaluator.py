import os
import traceback
import json
import dataclasses
import logging
import logging.config
from GPTClient import GPTClient
from JudgeSystem import JudgeSystem, JudgeStatus, JudgeAccepted, JudgeUnknownError

class SynthesizerEvaluator:
    '''
    Main class of the project.
    '''

    def __init__(self,
                 synthesizer,
                 prompt_wrapper,
                 response_wrapper,
                 model_name,
                 response_dir,
                 result_dir
                 ):
        '''
        :param synthesizer: Subclass of `AbstarctSynthesizer`, method `synthesize` should be implemented.
        :type synthesizer: Synthesizer

        :param prompt_wrapper: Subclass of `AbstarctPromptWrapper`, proporty `background`, `pre_prompt` and `post_prompt` should be implemented.
        :type prompt_wrapper: PromptWrapper

        :param response_wrapper: Subclass of `AbstractResponseWrapper`, method `wrap` should be implemented.
        :type response_wrapper: ResponseWrapper

        :param model_name: GPT model name.
        :type model_name: str

        :param response_dir: Folder path to save ChatGPT responses, should ensure it exists.
        :type response_dir: str

        :param result_dir: Folder path to save target code generated by the synthesizer, should ensure it exists.
        :type result_dir: str
        '''

        self.synthesizer        = synthesizer
        self.prompt_wrapper     = prompt_wrapper
        self.response_wrapper   = response_wrapper
        self.model_name         = model_name
        self.response_dir       = response_dir
        self.result_dir         = result_dir
        self.client             = GPTClient() 
        self.judge_system       = JudgeSystem(self.synthesizer) 
        self.logger             = logging.getLogger(__name__)

    def evaluate(self, task_name, data):
        '''
        Evaluate the synthesizer by one piece of data, including:
        1. Request ChatGPT for the synthesizer's DSL by wrapped prompts.
        2. Synthesize the DSL to generate target code, which can be called as `compile`.
        3. Judge whether the target code can satisfy the I/O specs in data.
        The result will be raised as `JudgeStatus` exception, just like status in common online judges.
        
        :param task_name: Identify the synthesizer and the data, used in log.
        :type task_name: str

        :param data:
        :type data: ProgramData

        :raise JudgeAccepted: The program passed all I/O spec tests.
        :raise JudgeWrongAnswer: The program failed at some I/O spec test.
        :raise JudgeTimeLimitExceeded: The program was executed for too long time, the time limit can be set in `judge_system`.
        :raise JudgeRuntimeError: The program crashed during being executed. 
        :raise JudgeCompileError: Error occured during synthesizing or getting target code. 
        :raise JudgeUnknownError: Other strange errors, eg: ChatGPT crashed.
        '''
        self.logger.info(f'{task_name}: requesting for {self.model_name}...')
        try:
            response = self.client.request(self.model_name,
                                      data.func_name,
                                      data.prompt, 
                                      os.path.join(self.response_dir, f"{task_name}.res"),
                                      self.prompt_wrapper,
                                      self.response_wrapper)
        except Exception :
            self.logger.exception("Exception")
            raise JudgeUnknownError("Unknown error occurs during requesting for ChatGPT!")
        self.logger.info(f'{task_name} request done!, the response {self.synthesizer.name} code is:\n{response}')
        save_path = os.path.join(self.result_dir, f"{task_name}.py")
        func = self.judge_system.compile(response, save_path, data)
        self.judge_system.judge(func, data.specs, data.func_name)
        raise JudgeAccepted("Accepted!")

    #TODO: Decouple clear and save.
    def evaluate_all(self, dataset, judge_status_path):
        '''
        Evaluate the synthesizer by dataset, save the results in `judge_status_path`.
        :param dataset:
        :type dataset: list[ProgramData]

        :param judge_status_path:
        :type judge_status_path: str
        '''
        self.judge_system.clear()
        try:
            for data in dataset:
                task_name = f"{self.synthesizer.name}_{data.prog_name}"
                try:
                    self.evaluate(task_name, data)
                except JudgeStatus as status:
                    self.logger.info(f'{task_name}: {str(status)}')
                    self.judge_system.add_judge_status(type(status).__name__, data)
                except Exception:
                    self.logger.exception(f'{task_name}: Unknown error occurs during judging!')
                    self.judge_system.add_judge_status("JudgeUnknownError", data)
        finally:
            with open(judge_status_path, 'w') as f:
                f.write(json.dumps(dataclasses.asdict(self.judge_system.judge_status_container)))

